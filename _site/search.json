[
  {
    "objectID": "posts/Module 5/index.html",
    "href": "posts/Module 5/index.html",
    "title": "Module # 5: Assignment: Matrix Algebra in R",
    "section": "",
    "text": "This is my Module #5 post for my LIS4370 blog. Welcome!\nCode repository:\n\nhttps://github.com/rwalshUSF/RProgrammingFall2025WalshRobert/tree/main/assignments/assignment-05-doing-math-1\nTask # 1 - Create matrices\n\n#In R, define\nA &lt;- matrix(1:100, nrow = 10)\nB &lt;- matrix(1:1000, nrow = 10)\n\nTask # 2 - Inspect dimensions\n\n#Verify whether each matrix is square\ndim(A) # should be 10 x 10\n\n[1] 10 10\n\ndim(B) # 10 x 100 - not square\n\n[1]  10 100\n\n\nTask # 3 - Compute inverse and determinant\n\n#Use solve() and det(); handle errors for non-square or singular matrices:\n\n#For A\ninvA &lt;- tryCatch(solve(A), error = function(e) print(e))\n\n&lt;simpleError in solve.default(A): Lapack routine dgesv: system is exactly singular: U[6,6] = 0&gt;\n\n# solve(A) does not work for my test! It produces an error!\n# Yes, the matrix is square, 10x10, however;\n# When I create matrix A, and use the solve() function the results produce\n# a calculation that states that Matrix A is exactly singular: U[6,6] = 0\n# This is perhaps due to strong correlations between the predictor variables:\ncor(A)\n\n      [,1] [,2] [,3] [,4] [,5] [,6] [,7] [,8] [,9] [,10]\n [1,]    1    1    1    1    1    1    1    1    1     1\n [2,]    1    1    1    1    1    1    1    1    1     1\n [3,]    1    1    1    1    1    1    1    1    1     1\n [4,]    1    1    1    1    1    1    1    1    1     1\n [5,]    1    1    1    1    1    1    1    1    1     1\n [6,]    1    1    1    1    1    1    1    1    1     1\n [7,]    1    1    1    1    1    1    1    1    1     1\n [8,]    1    1    1    1    1    1    1    1    1     1\n [9,]    1    1    1    1    1    1    1    1    1     1\n[10,]    1    1    1    1    1    1    1    1    1     1\n\n# I wrapped solve(A) in a try-catch function to get the program to run.\n\ndetA &lt;- det(A)\ndetA\n\n[1] 0\n\n# det(A) works, however, the output simply provides the 0 which indicates that\n# there is no inverse solution to this matrix, it is singular.\n\n#For B, use tryCatch to capture errors\ninvB &lt;- tryCatch(solve(B), error = function(e) print(e))\n\n&lt;simpleError in solve.default(B): 'a' (10 x 100) must be square&gt;\n\n# The operations on B fail because the dimensions of matrix B are 10x100\n# resulting in a non-square matrix that does not have an inverse solution.\n\ndetB &lt;- tryCatch(det(B), error = function(e) print(e))\n\n&lt;simpleError in determinant.matrix(x, logarithm = TRUE, ...): 'x' must be a square matrix&gt;\n\n# Therefore solve(B) will not produce a solution and det(B) will not as well.\n\nTask # 4 - Document your results\n\n# On your blog, include:\n\n# R code for creating A and B, and for computing invA, detA, invB, and detB\n# Shown Above.\n\n# Output or error messages for each operation.\n# Displayed after the erroneous function calls.\n\n# A brief explanation:\n\n# Why solve(A) and det(A) work.\n# Shown/explained above.\n\n# Any notes on numeric stability or performance.\n\n# Some notes on numeric stability include using randomized numbers that do\n# not match or correlate as closely. This will improve the performance of the\n# matrix linear algebra, and allow the functions/operations to compute\n# the correct results according to the proper matrix math, given that the\n# correct synax is provided in the code to perform the desired operations.\n# This can be considered when creating a linear model for bivariate and\n# multivariate linear regression and matrix math."
  },
  {
    "objectID": "posts/Module 3/index.html",
    "href": "posts/Module 3/index.html",
    "title": "Module # 3: Assignment: Introduction to data.frame",
    "section": "",
    "text": "This my Module #3 post for my LIS4370 blog. Welcome!\nTasks # 1 - Define and Inspect your data\n\n# In R, create vectors (correcting any syntax issues)\nName&lt;-c(\"Jeb\",\"Donald\",\"Ted\",\"Marco\",\"Carly\",\"Hillary\",\"Bernie\")\nABC_poll&lt;-c(4,62,51,21,2,14,15)\nCBS_poll&lt;-c(12,75,43,19,1,21,19)\n\n# Combine into a data frame\ndf_polls&lt;-data.frame(Name,ABC_poll,CBS_poll)\n\n# Use str() and head() to inspect your data frame\nstr(df_polls)\n\n'data.frame':   7 obs. of  3 variables:\n $ Name    : chr  \"Jeb\" \"Donald\" \"Ted\" \"Marco\" ...\n $ ABC_poll: num  4 62 51 21 2 14 15\n $ CBS_poll: num  12 75 43 19 1 21 19\n\nhead(df_polls)\n\n     Name ABC_poll CBS_poll\n1     Jeb        4       12\n2  Donald       62       75\n3     Ted       51       43\n4   Marco       21       19\n5   Carly        2        1\n6 Hillary       14       21\n\n\nTask # 2 - Compute Summary Statistics\n\n# Compute the mean, median, and range for each poll\n\n# mean for ABC poll\nmean(df_polls$ABC_poll)\n\n[1] 24.14286\n\n# median for ABC poll\nmedian(df_polls$ABC_poll)\n\n[1] 15\n\n# range for ABC poll\nrange(df_polls$ABC_poll)\n\n[1]  2 62\n\n# mean for CBS poll\nmean(df_polls$CBS_poll)\n\n[1] 27.14286\n\n# median for CBS poll\nmedian(df_polls$CBS_poll)\n\n[1] 19\n\n# range for CBS poll\nrange(df_polls$CBS_poll)\n\n[1]  1 75\n\n\n\n# Add a column for the difference between CBS and ABC\ndf_polls$Diff&lt;-df_polls$CBS_poll-df_polls$ABC_poll\n\nTasks # 3 & 4 - Discuss and Reflect after reading text and programming analysis\nOn your blog, write 2–3 paragraphs addressing:\nKey patterns you observe (e.g., which candidate shows the largest discrepancies).\nWhen looking at the data frame and the poll count values for each candidate, the data looks well organized at first with each candidates name and the estimated poll results side by side in each corresponding row for that candidate. The means for each poll are rather close with ABC_poll = ~24 and CBS_poll ~27 for a difference of ~3. The medians have slightly more variance as the ABC_polls = 15 and the CBS_polls = 19, a difference of 4. The ranges are close as well with the ABC_polls ranging from 2-62 and the CBS_polls from 1-75.\nWhen the ‘Diff’ column is calculated with CBS_poll-ABC_poll values: Donald shows the largest positive discrepancies and Ted shows the largest negative discrepancies.\nWhen the ‘Diff’ column is calculated with ABC_poll-CBS_poll values: Ted shows the largest positive discrepancies and Donald shows the largest negative discrepancies.\nImpact of using made‑up data—what limitations does this introduce?\nThe main impact and risks/limitations of using made up data seems to be the method chosen for evaluating the difference between the two polls. Depending on which poll value is subtracted from the other has a large impact on the final result. For ex. the results look much different in favor of one candidate for another when ABC_polls values are subtracted from CBS_polls values versus CBS_polls values are subtracted from ABC_polls values.\nHow you might collect or validate real poll data in a true analysis.\nOne strategy I would implement to collect or validate real poll data in a true analysis would be to label the difference specifically or to add a two bar visualization that shows the counts side by side to display the difference rather than using a subtraction method.\nInclude your R code (in a code chunk) and the generated plot.\n\nlibrary(ggplot2)\ndf_polls$Name&lt;-as.factor(df_polls$Name)\n\n# Here is the bar chart showing the difference between CBS_poll-ABC_poll\nggplot(df_polls,aes(x=Name,y=Diff, color=Diff, fill=Diff))+\n  geom_bar(stat = \"identity\")+\n  ylab(\"Difference between polls\")+\n  ggtitle(\"Difference between CBS_poll-ABC_poll\")\n\n\n\n\n\n\n\n# Re-calculate the Diff column for the difference between ABC and CBS\ndf_polls$Diff&lt;-df_polls$ABC_poll-df_polls$CBS_poll\n\n# Here is the bar chart showing the difference between ABC_poll-CBS_poll\nggplot(df_polls,aes(x=Name,y=Diff, color=Diff, fill=Diff))+\n  geom_bar(stat = \"identity\")+\n  ylab(\"Difference between polls\")+\n  ggtitle(\"Difference between ABC_poll-CBS_poll\")\n\n\n\n\n\n\n\n\nHere is a screenshot of my updated Github repo with the script and link to the new Module 3 blog post:\nhttps://rwalshusf.quarto.pub/httpsrwalshusfquartopubr-programming-journal---robert-walsh/posts/Module%203/"
  },
  {
    "objectID": "posts/Module 1/index.html",
    "href": "posts/Module 1/index.html",
    "title": "Module # 1: Assignment #1:",
    "section": "",
    "text": "This my first post for my LIS4370 blog. Welcome!\nTask # 1\nI have successfully created a GitHub repository named: r-programming-assignments\nHere is a screenshot of the repository and README.md:\n\nTasks # 2 & 3\nI have also created a blog titled “R Programming Journal – Robert Walsh”.\nHere is a screenshot of the blog and installed R Studio console:\n\nThe installation of R Studio was straightforward, mainly because I had already installed the program and IDE on my machine for a different course here at USF. One issue I had was connecting the blog project to the new GitHub repository. I resolved this problem by copying the commands that GitHub generated when the repository was created and then running them in the R Studio terminal. This worked well and connected the R Studio blog project to the Git Hub version control repository. A second issue that I encountered was software updates. I had not used R Studio for quite some time; therefore, I needed to complete some updates. After the updates were completed, I was able to use the software and the IDE. I am currently on a Windows 11 PC, using version ‘R 4.4.2’ and R Studio version ‘2024.12.0.467’.\nTask # 4\nWhen using the R programming language, vectors are fundamental objects or entities that serve as containers for storing data elements or data values. Vector container objects can be used in various ways to store data in the computer’s memory, and the vector addresses/information can be referenced at a later time, throughout the program to recall the values that are stored within them. Vectors are fundamental to data analysis in R because they have the ability and flexibility to have descriptive names that represent the data that they contain. The functions and applications in R Programming utilize the vector abstraction ability to enable programs to perform vector math and use vector elements as input parameters in functions. Results from calculations and analysis can be updated and stored in new or existing vectors, allowing further data analysis."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About Me",
    "section": "",
    "text": "UNIVERSITY OF SOUTH FLORIDA - SCHOOL OF INFORMATION STUDENT\nA blog for sharing my class assignments."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R Programming Journal - Robert Walsh",
    "section": "",
    "text": "Module # 5: Assignment: Matrix Algebra in R\n\n\n\n\n\n\n\n\n\n\n\nSep 22, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule # 4: Assignment: Programming Structure in R\n\n\n\n\n\n\n\n\n\n\n\nSep 15, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule # 3: Assignment: Introduction to data.frame\n\n\n\n\n\n\n\n\n\n\n\nSep 5, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule # 2: Assignment: Importing Data and Function Evaluation in R\n\n\n\n\n\n\n\n\n\n\n\nAug 29, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\n\n\n\n\n\n\nModule # 1: Assignment #1:\n\n\n\n\n\n\n\n\n\n\n\nAug 24, 2025\n\n\nRobert Walsh\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/Module 2/index.html",
    "href": "posts/Module 2/index.html",
    "title": "Module # 2: Assignment: Importing Data and Function Evaluation in R",
    "section": "",
    "text": "This my Module #2 post for my LIS4370 blog. Welcome!\nTasks # 1 & 2\nI have downloaded and reviewed the data import instructions. The ‘readr’ functions are more familiar to me than the ‘tidyr’ and I have experience reading and writing various data sets with .csv, .tsv, and html links. I have also read the assigned Chapters.\nTask # 3\nEvaluate the ‘myMean()’ function:\n\n# Use the vector:\n\nassignment2 &lt;- c(16, 18, 14, 22, 27, 17, 19, 17, 17, 22, 20, 22)\n\n# Consider the function:\n\nmyMean &lt;- function(assignment2) {\n  return(sum(assignment) / length(someData))\n}\n\n# Run myMean(assignment2) and record the output or error:\nmyMean(assignment2)\n\nError in myMean(assignment2): object 'assignment' not found\n\n# The result or error message from testing myMean(assignment2):\n\n\nThe function call for ‘myMean(assignment2)’ fails because the variable names inside the functions instructions are incorrect. The function takes an argument or input parameter of ‘assignment2’ and the instructions attempt to reference objects named ‘assignment’ in the sum() function and ‘someData’ in the length() function.\n\n# This is a corrected version of the myMean() function that correctly returns\n# the mean of 'assignment2'\n\nassignment2 &lt;- c(16, 18, 14, 22, 27, 17, 19, 17, 17, 22, 20, 22)\n\nmyMean &lt;- function(assignment2) {\n  return(sum(assignment2) / length(assignment2))\n}\n\n# Run myMean(assignment2) and record the output:\nmyMean(assignment2)\n\n[1] 19.25\n\n\nHere is a screenshot of my GitHub repository that shows the blog link and the corrected myMean function .R file: \nBlog link:\nhttps://rwalshusf.quarto.pub/httpsrwalshusfquartopubr-programming-journal---robert-walsh/"
  },
  {
    "objectID": "posts/Module 4/index.html",
    "href": "posts/Module 4/index.html",
    "title": "Module # 4: Assignment: Programming Structure in R",
    "section": "",
    "text": "This is my Module #4 post for my LIS4370 blog. Welcome!\nCode repository:\n\nhttps://github.com/rwalshUSF/RProgrammingFall2025WalshRobert/tree/main/assignments/assignment-04-programming-structure\nTasks # 1 - Data Preparation and Cleaning\n\n#Define each vector in R, converting categorical strings to numeric codes\n#(bad=1,good=0) (low=0,high=1) and handling NA's appropriately:\n\nFrequency&lt;-c(0.6,0.3,0.4,0.4,0.2,0.6,0.3,0.4,0.9,0.2)\nBloodPressure&lt;-c(103,87,32,42,59,109,78,205,135,176)\nFirstAssess&lt;-c(1,1,1,1,0,0,0,0,NA,1) ##(bad=1,good=0)\nSecondAssess&lt;-c(0,0,1,1,0,0,1,1,1,1) ##(low=0,high=1)\nFinalDecision&lt;-c(0,1,0,1,0,1,0,1,1,1) ##(low=0,high=1)\n\ndf_hosp&lt;-data.frame(Frequency,BloodPressure,FirstAssess,SecondAssess,FinalDecision,stringsAsFactors = FALSE)\n\n#Inspect and handle NA's\nsummary(df_hosp)\n\n   Frequency    BloodPressure     FirstAssess      SecondAssess FinalDecision\n Min.   :0.20   Min.   : 32.00   Min.   :0.0000   Min.   :0.0   Min.   :0.0  \n 1st Qu.:0.30   1st Qu.: 63.75   1st Qu.:0.0000   1st Qu.:0.0   1st Qu.:0.0  \n Median :0.40   Median : 95.00   Median :1.0000   Median :1.0   Median :1.0  \n Mean   :0.43   Mean   :102.60   Mean   :0.5556   Mean   :0.6   Mean   :0.6  \n 3rd Qu.:0.55   3rd Qu.:128.50   3rd Qu.:1.0000   3rd Qu.:1.0   3rd Qu.:1.0  \n Max.   :0.90   Max.   :205.00   Max.   :1.0000   Max.   :1.0   Max.   :1.0  \n                                 NA's   :1                                   \n\ndf_hosp&lt;-na.omit(df_hosp)\n# How handling of NA values affected your analysis:\n\n# There was one NA value in the data set and it changed from NA to '0' in the first\n# assessment column after applying the function to omit the NA value. This would\n# affect the results by placing one more patient into the 'good' blood pressure\n# category for the first assessment data.\ndf_hosp\n\n   Frequency BloodPressure FirstAssess SecondAssess FinalDecision\n1        0.6           103           1            0             0\n2        0.3            87           1            0             1\n3        0.4            32           1            1             0\n4        0.4            42           1            1             1\n5        0.2            59           0            0             0\n6        0.6           109           0            0             1\n7        0.3            78           0            1             0\n8        0.4           205           0            1             1\n10       0.2           176           1            1             1\n\n\nTask # 2 - Generate Basic Visualizations\n\n# How blood pressure varies with each Doctor's assessment and the final decision:\n\n# A. Side-by-Side Boxplots\n\nboxplot(BloodPressure~FirstAssess,data = df_hosp,names = c(\"Good\",\"Bad\"),ylab = \"Blood Pressure\",main = \"BP by First MD Assessment\")\n\n\n\n\n\n\n\n# The side-by-side boxplot for the first MD assessment does a good job at visualizing\n# the data frame with a chart that shows the minimum, maximum, and median blood\n# pressure measurements for the patients that have been labeled as having good\n# or bad blood pressure. The range from lower to upper quartiles is around 75-155\n# for good and 40-100 for bad with a median of around 85-95 on both. The overall\n# range for good measurements seems high at ~ 170.\n\nboxplot(BloodPressure~SecondAssess,data = df_hosp,names = c(\"Low\",\"High\"),ylab = \"Blood Pressure\",main = \"BP by Second MD Assessment\")\n\n\n\n\n\n\n\n# The side-by-side boxplot for the second MD assessment does a good job at visualizing\n# the data frame also showing the minimum, maximum, and median blood\n# pressure measurements for the patients that have been labeled as having low\n# or high blood pressure. The range from lower to upper quartiles shrinks to around\n# 75-110 for low and increases to 40-175 for high with the median remaining at\n# around 95 for low and decreasing to around 60-75 for high. The range for low\n# blood pressure  measurements is significantly lower at ~ 50.\n\nboxplot(BloodPressure~FinalDecision,data = df_hosp,names = c(\"Low\",\"High\"),ylab = \"Blood Pressure\",main = \"BP by Final Decision\")\n\n\n\n\n\n\n\n# The side-by-side boxplot for the final decision does a good job at visualizing\n# the data frame with a chart that shows the minimum, maximum, and median blood\n# pressure measurements for the patients that have been labeled as having low\n# or high blood pressure. The range from lower to upper quartiles shrinks to around\n# 50-95 for low and increases to 95-175 for high with the median dropping to\n# around 75 for low and increasing to around 105 for high\n\n# The final decision seems more conclusive and accurate with a narrower range\n# for low blood pressure and a wider range for high blood pressure.\n\n\n# B. Histograms\n# Visualize overall distributions of Frequency and Blood Pressure\nhist(df_hosp$Frequency,breaks = seq(0,1,by=0.1),xlab = \"Visit Frequency\",main = \"Histogram of Visit Frequency\")\n\n\n\n\n\n\n\nhist(df_hosp$BloodPressure,breaks = 8,xlab = \"Blood Pressure\",main = \"Histogram of Blood Pressure\")\n\n\n\n\n\n\n\n# Any notable patterns or outliers in the histograms:\n\n# When looking at the histograms, I noticed a pattern between visit frequency and\n# blood pressure levels that indicated the majority of patients visited between\n# 0.1 and 0.4 with blood pressure measurements of 50-110. The outliers seem to be\n# extreme blood pressure readings lower than 50 and higher than 150.\n\n# Potential clinical implications or limitations of this made up data:\n\n# One potential clinical implication or limitation of this made up data could be\n# extreme blood pressure measurements. I am not a medical professional, however\n# blood pressure measurements of 32 seem impossibly low for a patient to be alive.\n# Extremely high blood pressure measurements could be caused by the patients\n# mood or current state of being stressed out or very relaxed. Making up data\n# in this case could have dangerous health consequences for patients."
  }
]